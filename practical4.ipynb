{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-2pzsVf_4e6E"
   },
   "source": [
    "# DL Indaba Practical 4\n",
    "# Gated Recurrent Models (GRUs and LSTMs)\n",
    "*Developed by Stephan Gouws, Avishkar Bhoopchand & Ulrich Paquet.*\n",
    "\n",
    "**Introduction**\n",
    "\n",
    "So far we have looked at feedforward models which learn to map a single input **x** to a label (prediction) y. However, a lot of real-world data comes in the form of sequences, for example the words in natural languages, phonemes in speech, and so forth. In this practical we move from feed-forward models to ***sequence models*** which are designed to specifically model the **dependencies** between inputs in sequences of data that change over time. \n",
    "\n",
    "We will start with the basic/\"vanilla\" recurrent neural network (RNN) model. We will look at the intuition behind the model, and then approach it from a mathemetical as well as a programming point of view. Next we'll look at how to train RNNs, by discussing the Backpropagation-through-Time (BPTT) algorithm. Unfortunately, training RNNs still suffers from problems like vanishing and exploding grdients, so we will then look at how gated architectures overcome these issues. Finally we will implement an RNN and again apply this model to predict the labels of the handwritten MNIST images (yes! images can be thought of as **sequences of pixels**!). \n",
    "\n",
    "**Learning objectives**\n",
    "\n",
    "* Understanding how to use deep learning to deal with sequential input data.\n",
    "* Understanding how the vanilla RNN is the generalization of the DNN for sequential data.\n",
    "* Understanding the BPPT algorithm and the difficulties involved with training RNNs.\n",
    "* Understanding the GRU and LSTM architectures and how they help to solve some of these difficulties.\n",
    "\n",
    "**What is expected of you:**\n",
    " * Complete the RNN forward pass code by filling in the sections marked \"#IMPLEMENT-ME\" until the fprop check passes on the dummy data.\n",
    " * Fill in the derivatives for `cell_fn_backward()` and get `rnn_gradient_check()` to pass.\n",
    " * Fill in the missing pieces of `RNNSequenceClassifer` and train a reccurrent classifier on MNIST data.\n",
    " * Experiment with different cells and observe the effect on trainin time/accuracy.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YZTO9IIiWQNR"
   },
   "source": [
    "# Recurrent Neural Networks (RNNs)\n",
    "\n",
    "**NOTE**: You can safely skip the first section below if you know RNNs. Skim the second and dip into the third, but be sure to resurface back at \"Putting it all together\"!\n",
    "\n",
    "## The intuition\n",
    "\n",
    "RNNs generalize feedforward networks (FFNs) to be able to work with sequential data. FFNs take an input (e.g. an image) and immediately produce an output (e.g. a digit class).    RNNs, on the other hand, consider the data sequentially and remembers what it has seen in the past in order to make new predictions about the future observations.\n",
    "\n",
    "To understand this distinction, consider the example where we want to label words as the part-of-speech categories that they belong to: E.g. for the input sentence 'I want a duck' and 'He had to duck', we want our model to predict that duck is a noun in the first sentence and a verb in the second. To do this successfully, the model needs to be aware of the surrounding context.    However, if we feed a FFN model only one word at a time, how could it know the difference? If we want to feed it all the words at once, how do we deal with the fact that sentences are of different lengths? (We *could* try to find a trade-off by feeding it *windows of words*...)\n",
    "\n",
    "RNNs solve this issue by processing the sentence word-by-word, and maintaining an internal state summarizing what it has seen so far. This applies not only to words, but also to phonemes in speech, or even, as we will see, pixels of an image.\n",
    "\n",
    "## The RNN API\n",
    "\n",
    "Feedforward neural networks operate on vectors of fixed size. As we have done before, we could think of the \"[API](https://en.wikipedia.org/wiki/Application_programming_interface)\" of feedforward models as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "zBV_CSuvWPm5"
   },
   "outputs": [],
   "source": [
    "class FeedForwardModel():\n",
    "    \n",
    "    # ...\n",
    "    \n",
    "    def act_fn(self, x):\n",
    "        return np.tanh(x)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        '''One example of a FFN.'''\n",
    "        # Compute activations on the hidden layer.\n",
    "        hidden_layer = self.act_fn(np.dot(self.W_xh, x))\n",
    "            \n",
    "        # Compute the (linear) output layer activations.         \n",
    "        output = np.dot(self.W_ho, hidden_layer)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oU95UPGzWXOR"
   },
   "source": [
    "Recurrent neural networks (RNNs) generalize this idea to operating on **sequences of vectors**. To process sequences, the model has an internal **state** which gets updated with each new observation. Computationally, one can think of this as recursively applying a function `recurrent_fn` to update the state of the model based on each new input in the sequence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "FiNLIPNhWbiZ"
   },
   "outputs": [],
   "source": [
    "class RecurrentModel():\n",
    "    \n",
    "    # ...\n",
    "    def act_fn(self, x):\n",
    "        return np.tanh(x)\n",
    "\n",
    "    def recurrent_fn(self, x, prev_state):\n",
    "        '''Process the current input and the previous state and produce an output and a new state.'''\n",
    "        # Compute the new state based on the previous state and current input.    \n",
    "        new_state = self.act_fn(np.dot(self.W_hh, prev_state) + np.dot(self.W_xh, x))\n",
    "\n",
    "        # Compute the output vector.\n",
    "        y = np.dot(self.W_hy, new_state)\n",
    "\n",
    "        return new_state, y\n",
    "\n",
    "\n",
    "    def forward(self, data_sequence):\n",
    "\n",
    "        state = self.init_state()\n",
    "        all_states = [state]\n",
    "        last_output = None\n",
    "\n",
    "        for x in data_sequence:\n",
    "\n",
    "            new_state, last_output = recurrent_fn(x, state)\n",
    "            all_states.append(new_state)\n",
    "            state = new_state\n",
    "\n",
    "        return all_states, last_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w-DG9N5tEdaD"
   },
   "source": [
    "## Putting this together\n",
    "\n",
    "Look at the definition of `hidden_layer` in `FeedForwardModel.forward()` and `new_state` in `RecurrentModel.forward()`. If you're more comfortable with math, compare the expression for computing the hidden layer of a feedforward neural network where ($\\sigma$ is our non-linearity, tanh in the case shown above):\n",
    "\n",
    "*    $h = \\sigma(W_{xh}x)$\n",
    "\n",
    "to the expression for computing the hidden layer at time step $t$ in an RNN:\n",
    "    \n",
    "* $h_t = \\sigma(W_{hh}h_{t-1} + W_{xh}x_t)$\n",
    "\n",
    "**NOTE**: The weight subscript $W_{xz}$ is used to indicate a mapping from layer $x$ to layer $z$. \n",
    "    \n",
    "**QUESTIONS**:\n",
    " * How are they similar? \n",
    " * How are they different?\n",
    " * Why is $W_{hh}$ called \"recurrent\" weights?\n",
    " \n",
    "Spend a few min to think about and discuss this with your neighbour before you move on. \n",
    "\n",
    "## 'Unrolling' the network\n",
    "\n",
    "Imagine we are trying to classify sequences `X` into labels `y` (for now, let's keep it abstract). After running the `forward()` function of our RNN defined above on `X`, we would have a list of internal states of the model at each sequence position, and the final state of the network. This process is called **unrolling in time**, because you can think of it as unrolling the *computation graph* defined by the RNN `forward` function, over the inputs at each position of the sequence.    RNNs are often used to model **time series data**, and therefore these positions are referred to as **time-steps**, hence, \"unrolling over time\".\n",
    "\n",
    "![Unrolled RNN](images/rnn.jpg)\n",
    "\n",
    "> **We can therefore think of an RNN as a composition of identical feedforward neural networks (with replicated/tied weights), one for each moment or step in time. **\n",
    "\n",
    "These feedforward functions (i.e. our `recurrent_fn` above) are typically referred to as **cells**, and the only restriction on its API is that the cell function needs to be a differentiable function that can map an input and a state vector to an output and a new state vector. What we have shown above is called the **vanilla RNN**, but there are many more possibilities. In this practical, we will build up to a family of **gated-recurrent cells**. One of the most popular variants is called the **Long short-term memory** cell. But we're getting ahead of ourselves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kLweKkB7F4j4"
   },
   "source": [
    "## Training RNNs: (Truncated) Back-prop through Time\n",
    "\n",
    "RNNs model sequential data, and are designed to capture how ***outputs*** at the current time step are influenced by the ***inputs*** that came before them. This is referred to as **long-range dependencies**. At a high level, this allows the model to `remember` what it has seen so far in order to better contextualize what it is seeing at the moment (think about how knowing the context of the sentence or conversation can sometimes help one to better figure out the intended meaning of a misheard word or ambiguous statement). It is what makes these models so powerful, but it is also what makes them so hard to train!\n",
    "\n",
    "### BPTT: A quick theoretical overview\n",
    "\n",
    "The most well-known algorithm for training RNNs is called **back-propagation through time (BPTT)** (there are other algorithms). BPTT conceptually amounts to unrolling the computations of the RNN over time, computing the errors, and backpropagating the gradients through the unrolled graph structure.    Ideally we want to unroll the graph up to the maximum sequence length, however in practice, since sequence lengths vary and memory is limited, we only end up unrolling sequences up to some length $T$. This is called **truncated BPTT**, and is the most used variant of BPTT.\n",
    "\n",
    "At a high level, there are two main issues when using (truncated) BPTT to train RNNs:\n",
    "\n",
    "* Shared / tied recurrent weights ($W_{hh}$) mean that **the gradient on these weights at some time step $t$ depends on all time steps up to time-step $T$**, the maximum length of the unrolled graph. This also leads to the **vanishing/exploding gradients** problem.\n",
    "\n",
    "* As alluded to above, **memory usage grows linearly with the total number of steps $T$ that we unroll for**, because we need to save/cache the activations at each time-step.    This matters computationally, since memory is a limited resource. It also matters statistically, because it puts a limit on the types of dependencies the model is exposed to, and hence that it could learn.\n",
    "\n",
    "**NOTE**: Think about that last statement and make sure you understand those 2 points.\n",
    "\n",
    "BPTT is very similar to the standard back-propagation algorithm. Key to understanding the BPTT algorithm is to realize that gradients on the non-recurrent weights (weights of a per time-step classifier that tries to predict the next word in a sentence for example) and recurrent weights (that transform $h_{t-1}$ into $h_t$) are computed differently:\n",
    "\n",
    "* The gradients of **non-recurrent weights** ($W_{hy}$) depend only on the error at that time-step, $E_t$.\n",
    "* The gradients of **recurrent weights** ($W_{hh}$) depend on all time-steps up to maximum length $T$.\n",
    "\n",
    "The first point is fairly intuitive: predictions at time-step $t$ is related to the loss of that particular prediction. \n",
    "\n",
    "The second point will be explained in more detail in the lectures (see also [this great blog post](http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/)), but briefly, this can be summarized in these equations:\n",
    "\n",
    "1. The **current** state is a function of the **previous** state: $h_t = \\sigma(W_{hh}h_{t-1} + W_{xh}x_t)$\n",
    "2. The gradient of the loss $E_t$ at time $t$ on $W_{hh}$ is a function of the current hidden state and model predictions $\\hat{y}_t$ at time t: \n",
    "$\\frac{\\partial E_t}{\\partial W_{hh}} = \\frac{\\partial E_t}{\\partial \\hat{y}_t}\\frac{\\partial\\hat{y}_t}{\\partial h_t}\\frac{\\partial h_t}{\\partial W_{hh}}$\n",
    "3. Substituting (1) into (2) results in a **sum over all previous time-steps**:\n",
    "$\\frac{\\partial E_t}{\\partial W_{hh}} = \\sum\\limits_{k=0}^{t} \\frac{\\partial E_t}{\\partial \\hat{y}_t}\\frac{\\partial\\hat{y}_t}{\\partial h_t}\\frac{\\partial h_t}{\\partial h_k}\\frac{\\partial h_k}{\\partial W_{hh}}$\n",
    "\n",
    "Because of this **repeated multiplicative interaction**, as the sequence length $t$ gets longer, the gradients themselves can get diminishingly small (**vanish**) or grow too large and result in numeric overflow (**explode**). This has been shown to be related to the norms of the recurrent weight matrices being less than or equal to 1. Intuitively, it works very similar to how multiplying a small number $v<1.0$ with itself repeatedly can quickly go to zero, or conversely, a large number $v>1.0$ could quickly go to infinity; only this is for matrices.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9KRWSWRbGzBJ"
   },
   "source": [
    "To implement this, we need three components:\n",
    "\n",
    "* The code to fprop one time-step though the cell,\n",
    "* the code to fprop through the unrolled RNN,\n",
    "* the code to backprop one time-step through the cell, \n",
    "\n",
    "And then we'll put all this together within the BPTT algorithm. Let's start.\n",
    "\n",
    "### Forward Propagation\n",
    "\n",
    "Let's write the code to fprop one time-step though the cell. We'll need some helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "NINCPVs72WqW"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "## HELPER DEFINITIONS \n",
    "## NOTE: WE KEEP THESE EXPLICIT BECAUSE WE WILL NEED THEIR DERIVATIVES BELOW.\n",
    "\n",
    "def softmax(X):\n",
    "    eX = np.exp((X.T - np.max(X, axis=1)).T)\n",
    "    return (eX.T / eX.sum(axis=1)).T\n",
    "\n",
    "def cross_entropy(y_pred, y_train):\n",
    "    m = y_pred.shape[0]\n",
    "\n",
    "    prob = softmax(y_pred)\n",
    "    log_like = -np.log(prob[range(m), y_train])\n",
    "\n",
    "    data_loss = np.sum(log_like) / m\n",
    "\n",
    "    return data_loss\n",
    "\n",
    "def fc_forward(X, W, b):\n",
    "    '''A fully-connected feedforward layer.'''\n",
    "    out = np.dot(X, W) + b\n",
    "    cache = (W, X)\n",
    "    return out, cache\n",
    "\n",
    "def tanh_forward(X):\n",
    "    out = np.tanh(X)\n",
    "    cache = out\n",
    "    return out, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kikiBXVjIwG8"
   },
   "source": [
    "Now we can implement the equation $h_t = \\sigma(W_{hh}h_{t-1} + W_{xh}x_t)$ as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "Bw94oOd1I1Jj"
   },
   "outputs": [],
   "source": [
    "## Define the RNN Cell. We use a vanilla RNN.\n",
    "def cell_fn_forward(X, h, model, train=True):\n",
    "    Wxh, Whh, Why = model['Wxh'], model['Whh'], model['Why']\n",
    "    bh, by = model['bh'], model['by']\n",
    "\n",
    "    hprev = h.copy()\n",
    "\n",
    "    ## IMPLEMENT-ME: ... \n",
    "    h, h_cache = tanh_forward(np.dot(hprev, Whh) + np.dot(X, Wxh) + bh)\n",
    "    y, y_cache = fc_forward(h, Why, by)\n",
    "\n",
    "    cache = (X, Whh, h, hprev, y, h_cache, y_cache)\n",
    "\n",
    "    if not train:\n",
    "            # Compute per-time step outputs.\n",
    "            # NOTE: Here we build a classifer, but it could be anything else.\n",
    "            y = softmax(y)\n",
    "\n",
    "    return y, h, cache\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TRFfMGOVTt-C"
   },
   "source": [
    "Put this together to do the RNN fprop over the entire sequence:\n",
    "\n",
    "**QUESTION**: Notice how we save all activations in `caches`. Why do we need to do this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "XQUTNuJYTxqM"
   },
   "outputs": [],
   "source": [
    "def rnn_forward(X_train, y_train, model, initial_state, verbose=True):\n",
    "    ys = []\n",
    "    caches = []\n",
    "    loss = 0.\n",
    "\n",
    "    h = initial_state\n",
    "    t = 0\n",
    "    for x, y in zip(X_train, y_train):\n",
    "        \n",
    "        ## IMPLEMENT-ME: ... \n",
    "        y_pred, h, cache = cell_fn_forward(x, h, model, train=True)\n",
    "        loss += cross_entropy(y_pred, y)\n",
    "        ##\n",
    "        \n",
    "        ys.append(y_pred)\n",
    "        caches.append(cache)\n",
    "        \n",
    "        if verbose:\n",
    "            print \"Time-step: \", t\n",
    "            print \"x_t = \", x\n",
    "            print \"cur_state = \", h\n",
    "            print \"predicted y = \", y_pred\n",
    "            \n",
    "        t += 1\n",
    "    \n",
    "    # We return final hidden state, predictions, caches and final total loss.\n",
    "    return h, ys, caches, loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WLVIHIz1Kvug"
   },
   "source": [
    "Let's test this on some dummy data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "height": 731,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 536,
     "status": "ok",
     "timestamp": 1503667515621,
     "user": {
      "displayName": "Stephan Gouws",
      "photoUrl": "//lh4.googleusercontent.com/-6znVyM1oxdg/AAAAAAAAAAI/AAAAAAAAABI/vEPo2Ce7Rpc/s50-c-k-no/photo.jpg",
      "userId": "102606466886131565871"
     },
     "user_tz": -60
    },
    "id": "NQJ7DH-gJxbq",
    "outputId": "ef70162d-0e2d-4316-bf85-e8efcdd55b5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created dummy input data X\n",
      "Created fake targets:  [1 0 0 1 0]\n",
      "\n",
      "Running FPROP on dummy data: \n",
      "Time-step:  0\n",
      "x_t =  [[ 0.46439233 -3.56351666  1.32110562  0.15263055  0.16452954 -0.43009569\n",
      "   0.76736874  0.98491984]]\n",
      "cur_state =  [[ 0.96177185 -0.9949211  -0.32244779  0.73506912]]\n",
      "predicted y =  [[ 0.55862327  1.40120743 -1.08963098 -1.08400572 -0.28057832 -1.1015924\n",
      "  -0.11587845 -0.05159674]]\n",
      "Time-step:  1\n",
      "x_t =  [[ 0.27083585  1.39198619  0.07984231 -0.39996458 -1.02785056 -0.58471821\n",
      "   0.81659393 -0.08194705]]\n",
      "cur_state =  [[ 0.46602766  0.82754901  0.36055494 -0.95185866]]\n",
      "predicted y =  [[ 0.34742921 -1.29286866  2.47391689 -1.71784312 -1.5992273   1.24601455\n",
      "  -0.88538448 -0.11095221]]\n",
      "Time-step:  2\n",
      "x_t =  [[-0.34476601  0.52828815 -1.06898878 -0.51188131  0.29120536  0.5665337\n",
      "   0.50359176  0.28529568]]\n",
      "cur_state =  [[ 0.05420772  0.99026303  0.59787223 -0.29612811]]\n",
      "predicted y =  [[ 0.8010615  -1.61368801  2.09415036  0.17450844 -0.9812788   0.68753718\n",
      "  -0.38148559  0.05024376]]\n",
      "Time-step:  3\n",
      "x_t =  [[ 0.48428811  1.36348151 -0.78110528 -0.46801767  1.22457436 -1.28110828\n",
      "   0.8754755  -1.71071532]]\n",
      "cur_state =  [[ 0.54636226  0.72291255  0.98786587 -0.98647387]]\n",
      "predicted y =  [[ 0.99520914 -2.79581132  3.78816447 -2.78201096 -1.48485321  1.72724645\n",
      "  -1.47624028  0.1877338 ]]\n",
      "Time-step:  4\n",
      "x_t =  [[-0.4507651   0.74916381 -0.20393287 -0.18217541  0.680656   -1.81849899\n",
      "   0.04707164  0.39484421]]\n",
      "cur_state =  [[ 0.19868001  0.98286478  0.76491549 -0.91578737]]\n",
      "predicted y =  [[ 0.64388666 -2.30770109  3.12462806 -1.48085355 -1.28186319  1.50545244\n",
      "  -0.98578343  0.08359233]]\n",
      "Final hidden state:  [[ 0.19868001  0.98286478  0.76491549 -0.91578737]]\n",
      "\n",
      "============================\n",
      "Testing rnn_forward\n",
      "PASSED\n",
      "\n",
      "============================\n"
     ]
    }
   ],
   "source": [
    "# Create a helper function that calculates the relative error between two arrays\n",
    "def relative_error(x, y):\n",
    "    \"\"\" returns relative error \"\"\"\n",
    "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))\n",
    "\n",
    "# We set the seed to make the results reproducible.\n",
    "np.random.seed(1234)\n",
    "\n",
    "def _initial_state(hidden_dim):\n",
    "    return np.zeros((1, hidden_dim))\n",
    "\n",
    "def _init_model(input_dim, hidden_dim, output_dim):\n",
    "    D, H, C = input_dim, hidden_dim, output_dim # More compact.\n",
    "    model_params = dict(\n",
    "        Wxh=np.random.randn(D, H) / np.sqrt(D / 2.),\n",
    "        Whh=np.random.randn(H, H) / np.sqrt(H / 2.),\n",
    "        Why=np.random.randn(H, D) / np.sqrt(C / 2.),\n",
    "        bh=np.zeros((1, H)),\n",
    "        by=np.zeros((1, D)))\n",
    "    return model_params\n",
    "\n",
    "\n",
    "# Initialize model.\n",
    "input_dim=8\n",
    "hidden_dim=4\n",
    "output_dim=2    # num_classes\n",
    "num_steps = 5\n",
    "\n",
    "test_mdl = _init_model(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "# Create some dummy data (there's no batching, just num_steps input vectors)\n",
    "X_test = np.split(np.random.randn(num_steps, input_dim), num_steps, axis=0)\n",
    "y_test = np.random.randint(low=0, high=output_dim, size=num_steps).reshape(-1)\n",
    "#y_onehot = np.eye(output_dim)[y_ids]\n",
    "\n",
    "print \"Created dummy input data X\"\n",
    "print \"Created fake targets: \", y_test\n",
    "#print \"Created fake onehot targets: \\n\", y_onehot\n",
    "\n",
    "print \"\\nRunning FPROP on dummy data: \"\n",
    "\n",
    "initial_state = _initial_state(hidden_dim)\n",
    "last_state, ys, caches, loss = rnn_forward(X_test, y_test, test_mdl, initial_state, verbose=True)\n",
    "    \n",
    "print \"Final hidden state: \", last_state\n",
    "\n",
    "correct_final_state = np.array([[ 0.19868001,    0.98286478,    0.76491549, -0.91578737]])\n",
    "\n",
    "# Compare your output to the \"correct\" ones \n",
    "# The difference should be around 2e-8 (or lower)\n",
    "print \"\\n============================\"\n",
    "print 'Testing rnn_forward'\n",
    "diff = relative_error(last_state, correct_final_state)\n",
    "if diff <= 2e-8:\n",
    "    print 'PASSED'\n",
    "else:\n",
    "    print 'The difference of %s is too high, try again' % diff\n",
    "print \"\\n============================\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H3q0dGyzG6Ji"
   },
   "source": [
    "### Computing the derivative: Truncated BPTT \n",
    "\n",
    "Let's start with computing the per time-step derivative of `cell_fn_forward` wrt all model parameters. First, some helper derivative functions:\n",
    "\n",
    "**NOTE**: Make sure you understand how these were derived."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "rTCynUOYEg5i"
   },
   "outputs": [],
   "source": [
    "## HELPER DERIVATIVE FUNCTIONS\n",
    "def fc_backward(dout, cache):\n",
    "    W, h = cache\n",
    "\n",
    "    dW = np.dot(h.T, dout)\n",
    "    db = np.sum(dout, axis=0)\n",
    "    dX = np.dot(dout, W.T)\n",
    "\n",
    "    return dX, dW, db\n",
    "\n",
    "def tanh_backward(dout, cache):\n",
    "    dX = (1 - cache**2) * dout\n",
    "    return dX\n",
    "\n",
    "def dcross_entropy(y_pred, y_train):\n",
    "    m = y_pred.shape[0]\n",
    "    \n",
    "    grad_y = softmax(y_pred)\n",
    "    grad_y[range(m), y_train] -= 1.\n",
    "    grad_y /= m\n",
    "\n",
    "    return grad_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "evUka0sdM6ul"
   },
   "source": [
    "Let's put these together and write the code for $\\frac{\\partial E}{\\partial \\mathbf{\\theta}}$ for all parameters $\\mathbb{\\theta} = \\{W_{xh}, W_{hh}, W_{hy}, b_h, b_y\\}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "wo2xTk99M76j"
   },
   "outputs": [],
   "source": [
    "## PERFORM PER-TIMESTEP BACKWARD STEP\n",
    "## IMPLEMENT-ME: Most of this (with hints).\n",
    "\n",
    "def cell_fn_backward(y_pred, y_train, dh_next, cache):\n",
    "    X, Whh, h, hprev, y, h_cache, y_cache = cache\n",
    "\n",
    "    # Softmax gradient\n",
    "    dy = dcross_entropy(y_pred, y_train)\n",
    "\n",
    "    # Hidden to output gradient\n",
    "    dh, dWhy, dby = fc_backward(dy, y_cache)\n",
    "    dh += dh_next\n",
    "    dby = dby.reshape((1, -1))\n",
    "\n",
    "    # tanh\n",
    "    dh = tanh_backward(dh, h_cache)\n",
    "\n",
    "    # Hidden gradient\n",
    "    dbh = dh\n",
    "    dWhh = np.dot(hprev.T, dh)\n",
    "    dWxh = np.dot(X.T, dh)\n",
    "    dh_next = np.dot(dh, Whh.T)\n",
    "\n",
    "    grad = dict(Wxh=dWxh, Whh=dWhh, Why=dWhy, bh=dbh, by=dby)\n",
    "\n",
    "    return grad, dh_next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-6hdtyrPOCvH"
   },
   "source": [
    "Now let's put this together inside the BPTT algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "CHVwqUHvEmfa"
   },
   "outputs": [],
   "source": [
    "def bptt(model, X_train, y_train, initial_state):\n",
    "    \n",
    "    # Forward\n",
    "    last_state, ys, caches, loss = rnn_forward(X_train, y_train, model, initial_state)\n",
    "\n",
    "    loss /= y_train.shape[0]\n",
    "\n",
    "    # Backward\n",
    "    dh_next = np.zeros((1, last_state.shape[0]))\n",
    "    grads = {k: np.zeros_like(v) for k, v in model.items()}\n",
    "\n",
    "    for t in reversed(range(len(X_train))):\n",
    "        grad, dh_next = cell_fn_backward(ys[t], y_train[t], dh_next, caches[t])\n",
    "\n",
    "        for k in grads.keys():\n",
    "                grads[k] += grad[k]\n",
    "\n",
    "    for k, v in grads.items():\n",
    "        grads[k] = np.clip(v, -5., 5.)\n",
    "\n",
    "    return grads, loss, last_state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "55st2mH8ghcV"
   },
   "source": [
    "Finally, we can check our implementation using numerically-derived gradients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "eNSXgrwvgtYY"
   },
   "outputs": [],
   "source": [
    "def rnn_gradient_check(model, x, y, init_state, h=0.001, error_threshold=0.01):\n",
    "    \n",
    "    # Calculate the gradients using backpropagation. We want to checker if these are correct.\n",
    "    bptt_gradients, _, _ = bptt(model, x, y, init_state)\n",
    "    \n",
    "    # List of all parameters we want to check.\n",
    "    model_parameters = ['Wxh', 'Whh', 'Why', 'bh', 'by']\n",
    "    \n",
    "    # Gradient check for each parameter\n",
    "    for pidx, pname in enumerate(model_parameters):\n",
    "        # Get the actual parameter value from the model, e.g. model.W\n",
    "        parameter = model[pname]\n",
    "        \n",
    "        print \"Performing gradient check for parameter %s with size %d.\" % (pname, np.prod(parameter.shape))\n",
    "        \n",
    "        # Iterate over each element of the parameter matrix, e.g. (0,0), (0,1), ...\n",
    "        it = np.nditer(parameter, flags=['multi_index'], op_flags=['readwrite'])\n",
    "        \n",
    "        while not it.finished:\n",
    "            \n",
    "            ix = it.multi_index\n",
    "            # Save the original value so we can reset it later\n",
    "            original_value = parameter[ix]\n",
    "            \n",
    "            ## IMPLEMENT-ME: ...\n",
    "\n",
    "            # Estimate the gradient using (f(x+h) - f(x-h))/(2*h)\n",
    "            parameter[ix] = original_value + h\n",
    "            # Compute the ENTIRE rnn_foward, evaluate cross entropy loss\n",
    "            _, _, _, gradplus = rnn_forward(x, y, model, init_state, verbose=False)\n",
    "            parameter[ix] = original_value - h\n",
    "            _, _, _, gradminus = rnn_forward(x, y, model, init_state, verbose=False)\n",
    "            estimated_gradient = (gradplus - gradminus)/(2*h)\n",
    "            \n",
    "            ##\n",
    "            \n",
    "            # Reset parameter to original value\n",
    "            parameter[ix] = original_value\n",
    "            \n",
    "            # The gradient for this parameter calculated using backpropagation\n",
    "            backprop_gradient = bptt_gradients[pname][ix]\n",
    "            \n",
    "            # calculate The relative error: (|x - y|/(|x| + |y|))\n",
    "            relative_error = np.abs(backprop_gradient - estimated_gradient) / (np.abs(backprop_gradient) + np.abs(estimated_gradient))\n",
    "            \n",
    "            # If the error is to large fail the gradient check\n",
    "            if relative_error > error_threshold:\n",
    "                print \"Gradient Check ERROR: parameter=%s ix=%s\" % (pname, ix)\n",
    "                print \"+h Loss: %f\" % gradplus\n",
    "                print \"-h Loss: %f\" % gradminus\n",
    "                print \"Estimated_gradient: %f\" % estimated_gradient\n",
    "                print \"Backpropagation gradient: %f\" % backprop_gradient\n",
    "                print \"Relative Error: %f\" % relative_error\n",
    "                return\n",
    "            it.iternext()\n",
    "        \n",
    "        print \"Gradient check for parameter %s: PASSED.\" % (pname)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CtGlXaAZkEpV"
   },
   "source": [
    "Aaaaand let's test it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "height": 697,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 410,
     "status": "ok",
     "timestamp": 1503668261812,
     "user": {
      "displayName": "Stephan Gouws",
      "photoUrl": "//lh4.googleusercontent.com/-6znVyM1oxdg/AAAAAAAAAAI/AAAAAAAAABI/vEPo2Ce7Rpc/s50-c-k-no/photo.jpg",
      "userId": "102606466886131565871"
     },
     "user_tz": -60
    },
    "id": "yPt9o7pwkEVE",
    "outputId": "7137ac0e-93d0-4ea7-c9b7-96ccb1751379"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time-step:  0\n",
      "x_t =  [[ 0.46439233 -3.56351666  1.32110562  0.15263055  0.16452954 -0.43009569\n",
      "   0.76736874  0.98491984]]\n",
      "cur_state =  [[ 0.96177185 -0.9949211  -0.32244779  0.73506912]]\n",
      "predicted y =  [[ 0.55862327  1.40120743 -1.08963098 -1.08400572 -0.28057832 -1.1015924\n",
      "  -0.11587845 -0.05159674]]\n",
      "Time-step:  1\n",
      "x_t =  [[ 0.27083585  1.39198619  0.07984231 -0.39996458 -1.02785056 -0.58471821\n",
      "   0.81659393 -0.08194705]]\n",
      "cur_state =  [[ 0.46602766  0.82754901  0.36055494 -0.95185866]]\n",
      "predicted y =  [[ 0.34742921 -1.29286866  2.47391689 -1.71784312 -1.5992273   1.24601455\n",
      "  -0.88538448 -0.11095221]]\n",
      "Time-step:  2\n",
      "x_t =  [[-0.34476601  0.52828815 -1.06898878 -0.51188131  0.29120536  0.5665337\n",
      "   0.50359176  0.28529568]]\n",
      "cur_state =  [[ 0.05420772  0.99026303  0.59787223 -0.29612811]]\n",
      "predicted y =  [[ 0.8010615  -1.61368801  2.09415036  0.17450844 -0.9812788   0.68753718\n",
      "  -0.38148559  0.05024376]]\n",
      "Time-step:  3\n",
      "x_t =  [[ 0.48428811  1.36348151 -0.78110528 -0.46801767  1.22457436 -1.28110828\n",
      "   0.8754755  -1.71071532]]\n",
      "cur_state =  [[ 0.54636226  0.72291255  0.98786587 -0.98647387]]\n",
      "predicted y =  [[ 0.99520914 -2.79581132  3.78816447 -2.78201096 -1.48485321  1.72724645\n",
      "  -1.47624028  0.1877338 ]]\n",
      "Time-step:  4\n",
      "x_t =  [[-0.4507651   0.74916381 -0.20393287 -0.18217541  0.680656   -1.81849899\n",
      "   0.04707164  0.39484421]]\n",
      "cur_state =  [[ 0.19868001  0.98286478  0.76491549 -0.91578737]]\n",
      "predicted y =  [[ 0.64388666 -2.30770109  3.12462806 -1.48085355 -1.28186319  1.50545244\n",
      "  -0.98578343  0.08359233]]\n",
      "Performing gradient check for parameter Wxh with size 32.\n",
      "Gradient check for parameter Wxh: PASSED.\n",
      "Performing gradient check for parameter Whh with size 16.\n",
      "Gradient check for parameter Whh: PASSED.\n",
      "Performing gradient check for parameter Why with size 32.\n",
      "Gradient check for parameter Why: PASSED.\n",
      "Performing gradient check for parameter bh with size 4.\n",
      "Gradient check for parameter bh: PASSED.\n",
      "Performing gradient check for parameter by with size 8.\n",
      "Gradient check for parameter by: PASSED.\n"
     ]
    }
   ],
   "source": [
    "rnn_gradient_check(test_mdl, X_test, y_test, initial_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "esdB5QtJFixP"
   },
   "source": [
    "# Gated Cells (GRUs and LSTMs)\n",
    "\n",
    "Vanilla RNNs are very powerful sequence models. However, they are difficult to train, in large part due to the difficulties with getting gradients to propagate through all the time-steps without vanishing or exploding. If the gradient explodes then backpropagation will not work because we will get NaN values for the gradient contributions from earlier layers. The simplest trick to overcome this is called **gradient clipping** (see [Pascanu et al., 2013](http://www.jmlr.org/proceedings/papers/v28/pascanu13.pdf)). One basically rescales the gradients once their norms exceed a certain threshold. Dealing with vanishing gradients is trickier. Proper weight initialization helps to overcome this at the start of training (e.g [orthogonal initialization](https://www.tensorflow.org/api_docs/python/tf/orthogonal_initializer)), and there are regularization tricks for encouraging constant backwards error flow, which works for some tasks, but is not theoretically well-motivated.\n",
    "\n",
    "Gated models (we will look at the GRU and LSTM) modify the architecture of the RNN cell to ensure constant gradient propagation. The problem with the vanilla RNN is that its entire state is multiplicatively updated (overwritten) at every time-step (notice the $W_{hh}h_{t-1}$ term): \n",
    "\n",
    "$h_t = \\sigma(W_{hh}h_{t-1} ...)$\n",
    "\n",
    "Gated cells (GRUs/LSTMs) have two main new ideas:\n",
    "\n",
    "* Ensure **incremental state change** by updating state *additively*: $h_t = h_{t-1} + f(h_{t-1})$.\n",
    "* Control the update process by selectively **modulating** how much to keep/forget of the old, how much to read, and how much to write into the new state.\n",
    "\n",
    "These models modulate how much to throw away (forget) from the previous state when making a proposed new state, and then how much to read from the previous state and write to the output of the cell, by using **gates** (values between 0 and 1 which squash information flow to some extent; little neural networks, of course!). Gates are    vectors of *per-dimension interpolation scalars*: When you multiply some vector by a gate vector, you essentially control how much of that vector you \"let through\".    Below we show the generic equation for such a gate (they're all the same!):\n",
    "\n",
    "$g_t = \\sigma(W_g h_{t-1} + U_g x_t + b_g)$\n",
    "\n",
    "where $\\sigma(z) = 1 / (1+e^{-z})$ is the sigmoid function (i.e. $0 \\leq \\sigma(z) \\leq 1.$). \n",
    "\n",
    "Think about this for a second:\n",
    "\n",
    "* What are the inputs of this gate (model)?\n",
    "* What are the parameters of this gate (model)?\n",
    "* What does this remind you of? \n",
    "\n",
    "**NOTE**: Gates are just (vectors of) simple, logistic regression models which take inputs from the previous hidden layer $h_{t-1}$ and the current input layer $x_t$) and produce outputs between 0 and 1. \n",
    "\n",
    "Now let's use them to modulate the flow of information. We'll start with the GRU and build up to the LSTM.\n",
    "\n",
    "\n",
    "## The Gated Recurrent Unit (GRU)\n",
    "\n",
    "The GRU was introduced in 2014 by [Cho et al.](https://arxiv.org/pdf/1406.1078.pdf) -- almost 2 decades after the LSTM -- but we'll start here because the GRU is simpler than the LSTM and based on the same principle. It uses only two gates per cell, a reset gate $r_t$ and an update gate $z_t$ (not the same z from the previous practicals!). These are defined exactly like the generic gates above:\n",
    "\n",
    "\\begin{aligned}\n",
    " r_t &= \\sigma(W_r h_{t-1} + U_r x_t + b_r) \\\\\n",
    " z_t &= \\sigma(W_z h_{t-1} + U_z x_t + b_z) \\\\\n",
    "\\end{aligned}\n",
    "\n",
    "First, it uses the **reset** gate to control how much of the previous state is used in computing the new proposed state:\n",
    "\n",
    "$\\tilde{h_t} = \\phi(W(r_t \\circ h_{t-1}) + Ux_t + b)$\n",
    "\n",
    "(where $\\circ$ is element-wise multiplication). Then it ties \"reading\" and \"writing\" of the LSTM (below) into an **update** gate (by bounding their sums to 1, i.e. the more you read the less you write) when the new state is calculated:\n",
    "\n",
    "$h_t = z_t \\circ h_{t-1} + (1 - z_t)\\circ \\tilde{h_t}$\n",
    "\n",
    "Try to reconcile the equations with the following flow diagram of the same:\n",
    "\n",
    "![GRU Cell](images/NH_GRUCell.png)\n",
    "\n",
    "* What happens when the reset gate is high/low?\n",
    "* What happens when the update gate is high/low?\n",
    "* How do these two interact?\n",
    "* Why would this architecture be more powerful than a vanilla RNN?\n",
    "\n",
    "## The Long Short-Term Memory unit (LSTM)\n",
    "\n",
    "The LSTM was introduced in 1997 by [Hochreiter and Schmidhuber](http://www.bioinf.jku.at/publications/older/2604.pdf). There are several different architectual variations 'out there', but they all operate by maintaining a separate **memory vector** $c_t$ and a **state vector** $h_t$ (i.e. the model computes a tuple of vectors per time-step, not just a single vector). We'll just focus on the \"basic\" LSTM version for now (we'll call this `BasicLSTM` in line with the name used in TensorFlow which we'll get to in the implementation section). It uses three gates, the **input**, **output** and **forget** gates, traditionally denoted by their first letters:\n",
    "\n",
    "\\begin{aligned}\n",
    " i_t &= \\sigma(W_i h_{t-1} + U_i x_t + b_i) \\\\\n",
    " o_t &= \\sigma(W_o h_{t-1} + U_o x_t + b_o) \\\\\n",
    " f_t &= \\sigma(W_f h_{t-1} + U_f x_t + b_f) \\\\\n",
    "\\end{aligned}\n",
    "\n",
    "Don't be intimidated by these equations. We've seen them all above already in their generic form. They're all just doing the same thing (computationally, not functionally). Convince yourself of this, by answering the following:\n",
    "\n",
    "* What is the **same** between these equations?\n",
    "* What is **different** between them?\n",
    "\n",
    "Now let's step through the rest of the `BasicLSTM` Cell:\n",
    "\n",
    "1. First, the `BasicLSTM` Cell uses no gating to create the proposed new hidden state (original notation uses g, but I use tilde h):\n",
    "\n",
    "            $\\tilde{h}_t = \\phi(W h_{t-1}) + Ux_t + b)$\n",
    "\n",
    "2. Then it updates its internal memory to be a combination of the previous memory $c_{t-1}$ (multiplied/modulated by the forget gates $f_t$) and the new proposed state $\\tilde{h}_t$ (modulated by the input gates $i_t$):\n",
    "\n",
    "            $c_t = f_t \\circ c_{t-1} + i_t \\circ \\tilde{h}_t$\n",
    "\n",
    "            **Intuitively**: the model could choose to ignore old memory completely (if $f_t$ is all 1s), or ignore the newly proposed state completely ($i_t$ all 0s), but more likely it would learn to do something in-between. **QUESTION**: Think about how and why this behaviour would be encouraged (learned) during training? \n",
    "\n",
    "3. Finally, the *state* that is actually output by the cell is a gated version of the memory vector, squashed by a tanh (because not everything in the memory cell might be immediately useful to the surrounding network):\n",
    "\n",
    "            $ h_t = o_t \\circ \\phi(c_t) $\n",
    "\n",
    "*Phewwww*. We know.. there is a lot going on here! The following flow-diagram might help a bit to make this more clear:\n",
    "\n",
    "![LSTM Cell](images/NH_BasicLSTMCell.png)\n",
    "\n",
    "Look at the equations and at the flow-diagram, and then try to answer the following questions:\n",
    "\n",
    "* How is the LSTM similar to the GRU?\n",
    "* How is it different?\n",
    "* What is the function of the memory vector (think about edge cases, e.g. where the forget gate is set to all 1s)?\n",
    "* Is the LSTM theoretically more powerful than the GRU? If so, why?\n",
    "* What is the computational drawback to using LSTMs (think about the number of gates; these must be parameterized..)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B_eGri3h8St8"
   },
   "source": [
    "# Implementing Recurrent Models in TensorFlow\n",
    "\n",
    "In TensorFlow, we implement recurrent models using two building blocks:\n",
    "\n",
    " 1. A (graph) definition for the cell (you can use [those provided](https://www.tensorflow.org/versions/r1.0/api_guides/python/contrib.rnn#Base_interface_for_all_RNN_Cells), or you can write your own); and\n",
    " 2. A method to unroll the graph over the sequence (dynamic vs unrolled).\n",
    " \n",
    "First, TensorFlow provides implementations for many of the standard RNN cells. Poke around in the docs.\n",
    "\n",
    "Second, TensorFlow provides two different ways to implement the recurrence operations: **dynamic or static**. Basically, static unrolling prebuilds the entire unrolled RNN over the maximum number of time-steps. Dynamic unrolling dynamically creates the graph at each time-step, and saves the activations during the forward phase for the backward phase.\n",
    "\n",
    "**QUESTION**: Why do activations need to be saved during the forward phase? (HINT: Look at our use of `cache` in the Numpy code above).\n",
    "\n",
    "We will use dynamic unrolling: it uses less memory, and (counterintuitively), oftentimes it turns out to be faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "height": 153,
     "output_extras": [
      {
       "item_id": 3
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7571,
     "status": "ok",
     "timestamp": 1503909740164,
     "user": {
      "displayName": "Stephan Gouws",
      "photoUrl": "//lh4.googleusercontent.com/-6znVyM1oxdg/AAAAAAAAAAI/AAAAAAAAABI/vEPo2Ce7Rpc/s50-c-k-no/photo.jpg",
      "userId": "102606466886131565871"
     },
     "user_tz": -60
    },
    "id": "bk6ne9E9qFV2",
    "outputId": "df27795f-da32-45cd-a124-da5bc7dff337"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import functools\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-wnPo8X1gp1e"
   },
   "source": [
    "# An RNN Image Labeler\n",
    "\n",
    "Let's reuse the BaseSoftmaxClassifier from the previous practicals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "9rk6sOnPf286"
   },
   "outputs": [],
   "source": [
    "class BaseSoftmaxClassifier(object):\n",
    "    def __init__(self, input_size, output_size, l2_lambda):        \n",
    "        # Define the input placeholders. The \"None\" dimension means that the \n",
    "        # placeholder can take any number of images as the batch size. \n",
    "        self.x = tf.placeholder(tf.float32, [None, input_size], name='x')\n",
    "        self.y = tf.placeholder(tf.float32, [None, output_size], name='y')    \n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.l2_lambda = l2_lambda\n",
    "\n",
    "        self._all_weights = [] # Used to compute L2 regularization in compute_loss().\n",
    "        \n",
    "        # You should override these in your build_model() function.\n",
    "        self.logits = None\n",
    "        self.predictions = None\n",
    "        self.loss = None\n",
    "        \n",
    "        self.build_model()\n",
    "        \n",
    "    def get_logits(self):\n",
    "        return self.logits\n",
    "    \n",
    "    def build_model(self):\n",
    "        # OVERRIDE THIS FOR YOUR PARTICULAR MODEL.\n",
    "        raise NotImplementedError(\"Subclasses should implement this function!\")\n",
    "        \n",
    "    def compute_loss(self):\n",
    "        \"\"\"All models share the same softmax cross-entropy loss.\"\"\"\n",
    "        assert self.logits is not None    # Ensure that logits has been created! \n",
    "        data_loss = tf.reduce_mean(\n",
    "                tf.nn.softmax_cross_entropy_with_logits(logits=self.logits, labels=self.y))\n",
    "        reg_loss = 0.\n",
    "        for w in self._all_weights:\n",
    "            reg_loss += tf.nn.l2_loss(w)\n",
    "            \n",
    "        return data_loss + self.l2_lambda * reg_loss\n",
    "    \n",
    "    def accuracy(self):\n",
    "        # Calculate accuracy.\n",
    "        assert self.predictions is not None    # Ensure that pred has been created!\n",
    "        correct_prediction = tf.equal(tf.argmax(self.predictions, 1), tf.argmax(self.y, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZNEifoNh8H58"
   },
   "source": [
    "We override `build_model()` to build the graph for the RNN classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "TUZ5BZUiggm9"
   },
   "outputs": [],
   "source": [
    "class RecurrentClassifier(BaseSoftmaxClassifier):\n",
    "    def __init__(self, model_params):\n",
    "            \n",
    "        self.config = model_params\n",
    "\n",
    "        super(RecurrentClassifier, self).__init__(model_params['input_size'],\n",
    "                                                  model_params['output_size'],\n",
    "                                                  model_params['l2_lambda'])\n",
    "             \n",
    "\n",
    "    def build_model(self):\n",
    "                \n",
    "        assert self.config['num_steps'] * self.config['pixels_per_step'] == self.config['input_size']\n",
    "        # We break up the input images into num_steps groups of pixels_per_step\n",
    "        # pixels each.        \n",
    "        rnn_input = tf.reshape(self.x, [-1, \n",
    "                                        self.config['num_steps'], \n",
    "                                        self.config['pixels_per_step']])\n",
    "\n",
    "        ## IMPLEMENT-ME: ... \n",
    "\n",
    "        # Define the main RNN 'cell', that will be applied to each timestep.\n",
    "        cell = self.config['cell_fn'](self.config['memory_units'])\n",
    "        # NOTE: This is how we apply Dropout to RNNs.\n",
    "        cell = tf.contrib.rnn.DropoutWrapper(\n",
    "                cell, \n",
    "                output_keep_prob = self.config['dropout_keep_prob'])\n",
    "        cell = tf.contrib.rnn.MultiRNNCell(cells=[cell] * self.config['num_layers'],\n",
    "                                           state_is_tuple=True)\n",
    "        ##########\n",
    "                \n",
    "        outputs, state = tf.nn.dynamic_rnn(cell, \n",
    "                                           rnn_input, \n",
    "                                           dtype=tf.float32)\n",
    "                \n",
    "        # Transpose the cell to get the output from the last timestep for each batch.\n",
    "        output = tf.transpose(outputs, [1, 0, 2])\n",
    "        last_hiddens = tf.gather(output, int(output.get_shape()[0]) - 1)\n",
    "                \n",
    "        # Define weights and biases for output prediction.\n",
    "        out_weights = tf.Variable(tf.random_normal([self.config['memory_units'],\n",
    "                                                    self.config['output_size']]))\n",
    "        self._all_weights.append(out_weights)\n",
    "                \n",
    "        out_biases = tf.Variable(tf.random_normal([self.config['output_size']]))\n",
    "                \n",
    "        self.logits = tf.matmul(last_hiddens, out_weights) + out_biases\n",
    "                \n",
    "        self.predictions = tf.nn.softmax(self.logits)\n",
    "        self.loss = self.compute_loss()\n",
    "                \n",
    "        \n",
    "    def get_logits(self):\n",
    "        return self.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "wkk33_lk7Z-I"
   },
   "outputs": [],
   "source": [
    "class MNISTFraction(object):\n",
    "    \"\"\"A helper class to extract only a fixed fraction of MNIST data.\"\"\"\n",
    "    def __init__(self, mnist, fraction):\n",
    "        self.mnist = mnist\n",
    "        self.num_images = int(mnist.num_examples * fraction)\n",
    "        self.image_data, self.label_data = mnist.images[:self.num_images], mnist.labels[:self.num_images]\n",
    "        self.start = 0\n",
    "        \n",
    "    def next_batch(self, batch_size):\n",
    "        start = self.start\n",
    "        end = min(start + batch_size, self.num_images)\n",
    "        self.start = 0 if end == self.num_images else end\n",
    "        return self.image_data[start:end], self.label_data[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "Jid1Ejbj7PO8"
   },
   "outputs": [],
   "source": [
    "def train_tf_model(tf_model,                                     \n",
    "                   session,    # The active session.\n",
    "                   num_epochs,    # Max epochs/iterations to train for.\n",
    "                   batch_size=50,    # Number of examples per batch.\n",
    "                   keep_prob=1.0,    # (1. - dropout) probability, none by default.\n",
    "                   train_only_on_fraction=1.,    # Fraction of training data to use.\n",
    "                   optimizer_fn=None,    # TODO(sgouws): more correct to call this optimizer_obj\n",
    "                   report_every=1, # Report training results every nr of epochs.\n",
    "                   eval_every=1,    # Evaluate on validation data every nr of epochs.\n",
    "                   stop_early=True,    # Use early stopping or not.\n",
    "                   verbose=True): \n",
    "\n",
    "    # Get the (symbolic) model input, output, loss and accuracy.\n",
    "    x, y = tf_model.x, tf_model.y\n",
    "    loss = tf_model.loss\n",
    "    accuracy = tf_model.accuracy()\n",
    "\n",
    "    # Compute the gradient of the loss with respect to the model parameters \n",
    "    # and create an op that will perform one parameter update using the specific\n",
    "    # optimizer's update rule in the direction of the gradients.\n",
    "    if optimizer_fn is None:\n",
    "        optimizer_fn = tf.train.AdamOptimizer()\n",
    "    optimizer_step = optimizer_fn.minimize(loss)\n",
    "\n",
    "    # Get the op which, when executed, will initialize the variables.\n",
    "    init = tf.global_variables_initializer()\n",
    "    # Actually initialize the variables (run the op).\n",
    "    session.run(init)\n",
    "\n",
    "    # Save the training loss and accuracies on training and validation data.\n",
    "    train_costs = []\n",
    "    train_accs = []\n",
    "    val_costs = []\n",
    "    val_accs = []\n",
    "\n",
    "    if train_only_on_fraction < 1:\n",
    "        mnist_train_data = MNISTFraction(mnist.train, train_only_on_fraction)\n",
    "    else:\n",
    "        mnist_train_data = mnist.train\n",
    "    \n",
    "    prev_c_eval = 1000000\n",
    "    \n",
    "    # Main training cycle.\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        avg_cost = 0.\n",
    "        avg_acc = 0.\n",
    "        total_batch = int(train_only_on_fraction * mnist.train.num_examples / batch_size)\n",
    "\n",
    "        ## IMPLEMENT-ME: ...\n",
    "        # Loop over all batches.\n",
    "        for i in range(total_batch):\n",
    "            batch_x, batch_y = mnist_train_data.next_batch(batch_size)\n",
    "                            \n",
    "            # Run optimization op (backprop) and cost op (to get loss value),\n",
    "            # and compute the accuracy of the model.\n",
    "            feed_dict = {x: batch_x, y: batch_y}\n",
    "            if keep_prob < 1.:\n",
    "                feed_dict[\"keep_prob:0\"] = keep_prob\n",
    "                \n",
    "            _, c, a = session.run(\n",
    "                    [optimizer_step, loss, accuracy], feed_dict=feed_dict)\n",
    "                        \n",
    "            # Compute average loss/accuracy\n",
    "            avg_cost += c / total_batch\n",
    "            avg_acc += a / total_batch            \n",
    "        \n",
    "        train_costs.append((epoch, avg_cost))\n",
    "        train_accs.append((epoch, avg_acc))\n",
    "\n",
    "        # Display logs per epoch step\n",
    "        if epoch % report_every == 0 and verbose:\n",
    "            print \"Epoch:\", '%04d' % (epoch+1), \"Training cost =\", \\\n",
    "                        \"{:.9f}\".format(avg_cost)\n",
    "                \n",
    "        if epoch % eval_every == 0:\n",
    "            val_x, val_y = mnist.validation.images, mnist.validation.labels            \n",
    "            \n",
    "            feed_dict = {x : val_x, y : val_y}\n",
    "            if keep_prob < 1.:\n",
    "                feed_dict['keep_prob:0'] = 1.0\n",
    "                \n",
    "            c_eval, a_eval = session.run([loss, accuracy], feed_dict=feed_dict)\n",
    "            \n",
    "            if verbose:\n",
    "                print \"Epoch:\", '%04d' % (epoch+1), \"Validation acc=\", \\\n",
    "                            \"{:.9f}\".format(a_eval)\n",
    "                \n",
    "            if c_eval >= prev_c_eval and stop_early:\n",
    "                print \"Validation loss stopped improving, stopping training early after %d epochs!\" % (epoch + 1)\n",
    "                break\n",
    "                \n",
    "            prev_c_eval = c_eval\n",
    "                \n",
    "            val_costs.append((epoch, c_eval))\n",
    "            val_accs.append((epoch, a_eval))\n",
    "            \n",
    "    \n",
    "    print \"Optimization Finished!\"\n",
    "    return train_costs, train_accs, val_costs, val_accs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "5Gnj1bC67dwT"
   },
   "outputs": [],
   "source": [
    "# Helper functions to plot training progress.\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def my_plot(list_of_tuples):\n",
    "    \"\"\"Take a list of (epoch, value) and split these into lists of \n",
    "    epoch-only and value-only. Pass these to plot to make sure we\n",
    "    line up the values at the correct time-steps.\n",
    "    \"\"\"\n",
    "    plt.plot(*zip(*list_of_tuples))\n",
    "\n",
    "def plot_multi(values_lst, labels_lst, y_label, x_label='epoch'):\n",
    "    # Plot multiple curves.\n",
    "    assert len(values_lst) == len(labels_lst)\n",
    "    plt.subplot(2, 1, 2)\n",
    "    \n",
    "    for v in values_lst:\n",
    "        my_plot(v)\n",
    "    plt.legend(labels_lst, loc='upper left')\n",
    "    \n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(y_label)\n",
    "    plt.show()    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "height": 924,
     "output_extras": [
      {
       "item_id": 27
      },
      {
       "item_id": 28
      },
      {
       "item_id": 29
      },
      {
       "item_id": 30
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 201993,
     "status": "ok",
     "timestamp": 1503910211261,
     "user": {
      "displayName": "Stephan Gouws",
      "photoUrl": "//lh4.googleusercontent.com/-6znVyM1oxdg/AAAAAAAAAAI/AAAAAAAAABI/vEPo2Ce7Rpc/s50-c-k-no/photo.jpg",
      "userId": "102606466886131565871"
     },
     "user_tz": -60
    },
    "id": "SPfywldo94TN",
    "outputId": "acb1d77b-619c-4df3-e3bd-33fecb30b28d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gradients_impl.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 Training cost = 1.638705258\n",
      "Epoch: 0001 Validation acc= 0.924799979\n",
      "Epoch: 0002 Training cost = 1.184986905\n",
      "Epoch: 0002 Validation acc= 0.957000017\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def build_train_eval_and_plot(model_params, train_params, verbose=True):\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    m = RecurrentClassifier(model_params)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        # Train model on the MNIST dataset.\n",
    "     \n",
    "        train_losses, train_accs, val_losses, val_accs = train_tf_model(\n",
    "                m, \n",
    "                sess,\n",
    "                verbose=verbose,\n",
    "                **train_params) \n",
    "        \n",
    "        # Now evaluate it on the test set:\n",
    "    \n",
    "        accuracy_op = m.accuracy()    # Get the symbolic accuracy operation\n",
    "        # Calculate the accuracy using the test images and labels.\n",
    "        accuracy = accuracy_op.eval({m.x: mnist.test.images, \n",
    "                                                                 m.y: mnist.test.labels})    \n",
    "        \n",
    "        if verbose: \n",
    "            print \"Accuracy on test set:\", accuracy\n",
    "            # Plot losses and accuracies.\n",
    "            plot_multi([train_losses, val_losses], ['train', 'val'], 'loss', 'epoch')\n",
    "            plot_multi([train_accs, val_accs], ['train', 'val'], 'accuracy', 'epoch')\n",
    "            \n",
    "        \n",
    "        ret = {'train_losses': train_losses, 'train_accs' : train_accs,\n",
    "                     'val_losses' : val_losses, 'val_accs' : val_accs,\n",
    "                     'test_acc' : accuracy}\n",
    "        \n",
    "        return m, ret\n",
    "\n",
    "#################################CODE TEMPLATE##################################\n",
    "# Specify the model hyperparameters:\n",
    "model_params = {\n",
    "        'input_size' : 784,\n",
    "        'output_size' : 10,\n",
    "        'batch_size'    : 100,\n",
    "        'num_steps' : 28,                    \n",
    "        'pixels_per_step' : 28,    # NOTE: num_steps * pixels_per_step must = input_size\n",
    "        'cell_fn' : tf.contrib.rnn.BasicRNNCell,\n",
    "        'memory_units' : 256,\n",
    "        'num_layers' : 1,\n",
    "        'l2_lambda' : 1e-3,\n",
    "        'dropout_keep_prob': 1.\n",
    "}\n",
    "\n",
    "\n",
    "# Specify the training hyperparameters:\n",
    "training_params = {\n",
    "        'num_epochs' : 100,     # Max epochs/iterations to train for.\n",
    "        'batch_size' : 100,    # Number of examples per batch, 100 default.\n",
    "        #'keep_prob' : 1.0,    # (1. - dropout) probability, none by default.\n",
    "        'train_only_on_fraction' : 1.,    # Fraction of training data to use, 1. for everything.\n",
    "        'optimizer_fn' : None,    # Optimizer, None for Adam.\n",
    "        'report_every' : 1, # Report training results every nr of epochs.\n",
    "        'eval_every' : 1,     # Evaluate on validation data every nr of epochs.\n",
    "     'stop_early' : True,    # Use early stopping or not.\n",
    "}\n",
    "\n",
    "# Build, train, evaluate and plot the results!\n",
    "trained_model, training_results = build_train_eval_and_plot(\n",
    "        model_params, \n",
    "        training_params, \n",
    "        verbose=True    # Modify as desired.\n",
    ")\n",
    "\n",
    "###############################END CODE TEMPLATE################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H-GfLCrGUwsD"
   },
   "source": [
    "## Exercise:    Try out different cells!\n",
    "\n",
    "Replace the BasicRNN cell with the BasicLSTMCell. What is the effect on accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "HVLm6vflU5v8"
   },
   "outputs": [],
   "source": [
    "# Your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hIAnbWvA-uDq"
   },
   "source": [
    "## Known good settings\n",
    "\n",
    "We got **98.8%** with this model and hyperparams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "btJIqQJ5-05c"
   },
   "outputs": [],
   "source": [
    "# Specify the model hyperparameters:\n",
    "model_params = {\n",
    "        'input_size' : 784,\n",
    "        'output_size' : 10,\n",
    "        'batch_size'    : 100,\n",
    "        'num_steps' : 28,                    \n",
    "        'pixels_per_step' : 28,    # NOTE: num_steps * pixels_per_step must = input_size\n",
    "        'cell_fn' : tf.contrib.rnn.BasicLSTMCell,\n",
    "        'memory_units' : 128,\n",
    "        'num_layers' : 1,\n",
    "        'l2_lambda' : 1e-3,\n",
    "        'dropout_keep_prob': 1.\n",
    "}\n",
    "\n",
    "\n",
    "# Specify the training hyperparameters:\n",
    "training_params = {\n",
    "        'num_epochs' : 100,     # Max epochs/iterations to train for.\n",
    "        'batch_size' : 100,    # Number of examples per batch, 100 default.\n",
    "        #'keep_prob' : 1.0,    # (1. - dropout) probability, none by default.\n",
    "        'train_only_on_fraction' : 1.,    # Fraction of training data to use, 1. for everything.\n",
    "        'optimizer_fn' : None,    # Optimizer, None for Adam.\n",
    "        'report_every' : 1, # Report training results every nr of epochs.\n",
    "        'eval_every' : 1,     # Evaluate on validation data every nr of epochs.\n",
    "     'stop_early' : True,    # Use early stopping or not.\n",
    "}\n",
    "\n",
    "# Build, train, evaluate and plot the results!\n",
    "trained_model, training_results = build_train_eval_and_plot(\n",
    "        model_params, \n",
    "        training_params, \n",
    "        verbose=True    # Modify as desired.\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EJdPMn5BT9m6"
   },
   "source": [
    "# NB: Before you go (5min)\n",
    "\n",
    "Pair up with someone else and go through the questions in \"Learning Objectives\" at the top. Take turns explaining each of these to each other, and be sure to ask the tutors if you're both unsure!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HEHT4Ne4b07g"
   },
   "source": [
    "# Resources\n",
    "\n",
    "* https://r2rt.com/written-memories-understanding-deriving-and-extending-the-lstm.html\n",
    "* HipsterNet Code: https://github.com/wiseodd/hipsternet/blob/master/hipsternet/neuralnet.py\n",
    "* http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-2-implementing-a-language-model-rnn-with-python-numpy-and-theano/\n",
    "* http://peterroelants.github.io/posts/rnn_implementation_part01/\n",
    "* http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XlWDP3jb37As"
   },
   "source": [
    "# Feedback\n",
    "\n",
    "Please send any bugs and comments to dli-practicals@googlegroups.com."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "last_runtime": {
    "build_target": "",
    "kind": "local"
   },
   "name": "Practical 4: Gated Recurrent Models (Solution)",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
